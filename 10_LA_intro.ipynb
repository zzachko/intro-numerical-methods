{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table>\n",
    " <tr align=left><td><img align=left src=\"./images/CC-BY.png\">\n",
    " <td>Text provided under a Creative Commons Attribution license, CC-BY. All code is made available under the FSF-approved MIT license. (c) Kyle T. Mandli</td>\n",
    "</table>\n",
    "\n",
    "Note:  This material largely follows the text \"Numerical Linear Algebra\" by Trefethen and Bau (SIAM, 1997) and is meant as a guide and supplement to the material presented there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%precision 3\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical Linear Algebra\n",
    "\n",
    "Numerical methods for linear algebra problems lies at the heart of many numerical approaches and is something we will spend some time on.  Roughly we can break down problems that we would like to solve into three general problems, solving a system of equations\n",
    "\n",
    "$$\n",
    "A \\mathbf{x} = \\mathbf{b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Projection problems or Linear Least Squares\n",
    "\n",
    "$$ A^TA\\mathbf{x} = A^T\\mathbf{b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and solving the eigenvalue problem\n",
    "\n",
    "$$A \\mathbf{v} = \\lambda \\mathbf{v}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We examine each of these problems separately and will evaluate some of the fundamental properties and methods for solving these problems. We will be careful in deciding how to evaluate the results of our calculations and try to gain some understanding of when and how they fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Factorizations\n",
    "\n",
    "For each fundamental problem, there are a range of algorithms to solve them, but all of these algorithms can often be succinctly described in terms of a matrix \"factorization\", the ability to write an arbitrary matrix $A$ as a product of matrices with special properties (e.g. triangular, diagonal, orthogonal) that make solving the general problem straightforward.  For example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table width=\"80%\">\n",
    "    <tr align=\"left\"><th>Problem</th> <th align=\"center\">Algorithms</th> <th align=\"center\">\"Factorizations\"</th></tr>\n",
    "     <tr align=\"left\"><td>$$A \\mathbf{x} = \\mathbf{b}$$</td> <td align=\"left\">Gaussian-Elimination, Gauss-Jordan Elimination</td> <td align=\"center\">$PA=LU$, $A=ER$</td></tr>\n",
    "    <tr align=\"center\"><td>$$ A^TA\\mathbf{x} = A^T\\mathbf{b}$$</td> <td align=\"center\">Orthogonalization algorithms (Gram-Schmidt, Modified GS, Householder, Givens)</td> <td align=\"center\">$A=QR$</td></tr>\n",
    "    <tr align=\"center\"><td>$$A \\mathbf{v} = \\lambda \\mathbf{v}$$</td> <td align=\"center\">Various Iterative methods (Power, inverse power, $RQ$ with shifts$\\ldots$</td> <td align=\"center\">$A=S\\Lambda S^{-1}$, $A=Q\\Lambda Q^T$, $A=MJM^{-1}$</td></tr>\n",
    "    <tr align=\"center\"><td align=\"left\">All of the above</td> <td align=\"center\">Singular Value Decomposition (SVD)</td> <td align=\"center\">$A = U\\Sigma V^T$</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To develop all of these algorithms in depth is a course in itself, however, here we will highlight some of the key factorizations and algorithms used in modern computational linear algebra.  In particular we will highlight issues of accuracy, stability and computational cost particularly in the limit of large systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some Example Problems\n",
    "\n",
    "The number and power of the different tools made available from the study of linear algebra makes it an invaluable field of study. Before we dive in to numerical approximations we first consider some of the pivotal problems that numerical methods for linear algebra are used to address.\n",
    "\n",
    "For this discussion we will be using the common notation $m \\times n$ to denote the dimensions of a matrix $A$.  The $m$ refers to the number of rows and $n$ the number of columns.  If a matrix is square, i.e. $m = n$, then we will use the notation that $A$ is $m \\times m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Systems of Equations\n",
    "\n",
    "The first type of problem is to find the solution to a linear system of equations.  If we have $m$ equations for $m$ unknowns it can be written in matrix/vector form,\n",
    "\n",
    "$$A \\mathbf{x} = \\mathbf{b}.$$\n",
    "\n",
    "For this example $A$ is an $m \\times m$ matrix, denoted as being in $\\mathbb{R}^{m\\times m}$, and $\\mathbf{x}$ and $\\mathbf{b}$ are column vectors with $m$ entries, denoted as $\\mathbb{R}^m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example 1:  Polynomial Fitting\n",
    "In our previous work on interpolation we found that the unique interpolating polynomial of order $n$ through the $n+1$ points $(x_i, y_i)$ can be found by solving a Linear system of equations \n",
    "\n",
    "$$\n",
    "V[\\phi_j(\\mathbf{x})]\\mathbf{w} = \\mathbf{y}\n",
    "$$\n",
    "\n",
    "where $V$ is a VanderMonde matrix whose columns are the basis functions $\\phi_j(\\mathbf{x})$ (e.g. the monomials) and $\\mathbf{w}\\in\\mathbb{R}^{n+1}$ are the coefficients (weights) such that the interpolating polynomial is given uniquely by\n",
    "$$\n",
    "    \\cal{P}_n(x) = \\sum_{j=0}^n w_j\\phi_j(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples 2: Solution of Non-linear equations by Newton's Method\n",
    "\n",
    "Given a non-linear system of equations \n",
    "\n",
    "$$\\mathbf{F}(\\mathbf{x})=\\mathbf{0}$$\n",
    "\n",
    "Newton's method provides an iterative method to find roots where at every step of the iteration a linear system of equations\n",
    "\n",
    "$$\n",
    "    J(\\mathbf{x_k})\\boldsymbol{\\delta}_k = -\\mathbf{F}(\\mathbf{x_k})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linear least squares\n",
    "\n",
    "In a similar case as above, say we want to fit a particular function (could be a polynomial) to a given number of data points except in this case we have more data points than free parameters.  In the case of polynomials this could be the same as saying we have $m$ data points but only want to fit a $n - 1$ order polynomial through the data where $n - 1 \\leq m$.  One of the common approaches to this problem is to minimize the \"least-squares\" error between the data and the resulting function:\n",
    "$$\n",
    "    E = \\left( \\sum^m_{i=1} |y_i - f(x_i)|^2 \\right )^{1/2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "E.g. Consider fitting the function \n",
    "$$\n",
    "    f(x) = w_1 + w_2 x + w_3 e^x\n",
    "$$ \n",
    "\n",
    "data that has random noise added to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If our function $f(x)$ can be written as a linear combination of basis functions\n",
    "\n",
    "$$\n",
    "    f(x) = \\sum_{j=1}^n w_j\\phi_j(x)\n",
    "$$\n",
    "\n",
    "then the linear system through $m$ points becomes $A\\mathbf{w} = \\mathbf{y}$ where $A$ is a generalized $m\\times n$ vandermonde matrix\n",
    "\n",
    "$$\n",
    "    A = \\begin{bmatrix}\n",
    "        \\phi_1(x_1) & \\phi_2(x_1) & \\cdots & \\phi_n(x_1) \\\\\n",
    "        \\phi_1(x_2) & \\phi_2(x_2) & \\cdots &  \\phi_n(x_2) \\\\\n",
    "        \\vdots & \\vdots & &\\vdots   \\\\\n",
    "        \\phi_1(x_m) & \\phi_2(x_m) & \\cdots &  \\phi_n(x_m) \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Turns out if we solve the system\n",
    "\n",
    "$$A^T A \\mathbf{x} = A^T \\mathbf{b}$$\n",
    "\n",
    "we can guarantee that the error is minimized in the least-squares sense[<sup>1</sup>](#footnoteRegression). (Although we will also show that this is not the most numerically stable way to solve this problem)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Define the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "x = numpy.linspace(-1.0, 1.0, N)\n",
    "y = x + numpy.random.random((N)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Define basis functions and Vandermonde matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# define the basis functions\n",
    "phi_1 = lambda x: numpy.ones(x.shape)\n",
    "phi_2 = lambda x: x\n",
    "phi_3 = lambda x: numpy.exp(x)\n",
    "# Define various Vandermonde matrix based on our x-values\n",
    "A = numpy.array([ phi_1(x), phi_2(x), phi_3(x)]).T\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Determine the weights of our linear function\n",
    "# result in the smallest sum of the squares of the residual.\n",
    "w = numpy.linalg.solve(numpy.dot(A.T, A), numpy.dot(A.T, y))\n",
    "error = y - A.dot(w)\n",
    "print('w = {}, ||e|| = {}'.format(w,numpy.linalg.norm(error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Do the same using numpy's lstsq routine (which solves a more numerically stable problem)\n",
    "w = numpy.linalg.lstsq(A,y, rcond=None)[0]\n",
    "error = y - A.dot(w)\n",
    "print('w = {}, ||e|| = {}'.format(w,numpy.linalg.norm(error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Plot it out, cuz pictures are fun!\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "f = A.dot(w)\n",
    "axes.plot(x, y, 'ko')\n",
    "axes.plot(x, f, 'r')\n",
    "axes.set_title(\"Least Squares Fit to Data\")\n",
    "axes.set_xlabel(\"$x$\")\n",
    "axes.set_ylabel(\"$f(x)$ and $y_i$\")\n",
    "axes.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Eigenproblems\n",
    "\n",
    "Eigenproblems come up in a variety of contexts and often are integral to many problem of scientific and engineering interest. It is such a powerful idea that it is not uncommon for us to take a problem and convert it into an eigenproblem. \n",
    "\n",
    "One of my favorite examples [The Tacoma Narrows Bridge Collapse](https://www.youtube.com/watch?v=XggxeuFDaDU)\n",
    "\n",
    "Or the original Google [Page-Rank](https://en.wikipedia.org/wiki/PageRank) algorithm which essentially finds the dominant eigenvector of an enormous sparse Markov Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here we introduce the idea and give some examples.\n",
    "\n",
    "As a review, if $A \\in \\mathbb{C}^{m\\times m}$ (a square matrix with complex values), a non-zero vector $\\mathbf{v}\\in\\mathbb{C}^m$ is an **eigenvector** of $A$ with a corresponding **eigenvalue** $\\lambda \\in \\mathbb{C}$ if \n",
    "\n",
    "$$A \\mathbf{v} = \\lambda \\mathbf{v}.$$\n",
    "\n",
    "One way to interpret the eigenproblem is that we are attempting to ascertain the \"action\" of the matrix $A$ on some subspace of $\\mathbb{C}^m$ where this action acts like scalar multiplication.  This subspace is called an **eigenspace**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### General idea of EigenProblems\n",
    "\n",
    "Rewriting the standard Eigen problem $A\\mathbf{v}=\\lambda\\mathbf{v}$ for $A \\in \\mathbb{C}^{m\\times m}$, $\\mathbf{v}\\in\\mathbb{C}^m$ as\n",
    "\n",
    "$$\n",
    "    (A - \\lambda I)\\mathbf{v} = \\mathbf{0}\n",
    "$$ \n",
    "\n",
    "it becomes clear that for $\\mathbf{v}$ to be non-trivial (i.e. $\\neq \\mathbf{0}$), requires that the matrix $(A-\\lambda I)$ be singular,  \n",
    "\n",
    "This is equivalent to finding all values of $\\lambda$ such that $|A-\\lambda I| = 0$ (the determinant of singular matrices is always zero).  However, it can also be shown that \n",
    "\n",
    "$$\n",
    "   | A-\\lambda I| = P_m(\\lambda)\n",
    "$$\n",
    "\n",
    "which is a $m$th order polynomial in $\\lambda$.  Thus $P_m(\\lambda)=0$ implies the eigenvalues are the $m$ roots of $P$, and the **eigenspace** corresponding to $\\lambda_i$ is just $N(A-\\lambda_i I)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Solving EigenProblems\n",
    "\n",
    "The temptation (and what we usually teach in introductory linear algebra classes) is to find the roots of $P_m(\\lambda)$  to get the eigenvalues, then find the null-space of $A-\\lambda I$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However that would be **wrong**.  The best algorithms for finding Eigenvalues are completely unrelated to rootfinding as we shall see (and in fact, the way you find the roots of polynomials is to find the eigenvalues of a \"companion matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Singular Value Decomposition\n",
    "\n",
    "One of the most beautiful, and useful, factorizations is the Singular Value Decomposition (or SVD),  \n",
    "\n",
    "$$\n",
    "    A = U\\Sigma V^T\n",
    "$$\n",
    "\n",
    "where $A$ is a general $m\\times n$ matrix, $U$ and $V$ are unitary (orthogonal) matrices such that $U^TU=I^{m\\times m}$, $V^TV = I^{n\\times n}$ and $\\Sigma$ is a diagonal matrix with real, positive  diagonal entries (the singular values)\n",
    "\n",
    "$$\n",
    "    \\sigma_1 \\geq \\sigma_2\\geq \\ldots \\sigma_{r} > 0\n",
    "$$\n",
    "where $r$ is the rank of $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Applications of the SVD\n",
    "\n",
    "The SVD combines all the aspects of basic linear algebra and is used in a large number of applications including\n",
    "\n",
    "* diagnosing ill-conditioned matrices\n",
    "* providing orthonormal bases for the 4-subspaces of $A$\n",
    "* Solving linear systems and linear least-squares problems\n",
    "* Solving ill-conditioned and singular linear systems (Pseudo-inverse)\n",
    "* Dimensional reduction in Data analysis (PCA, EOF, POD analysis)\n",
    "* and more...\n",
    "\n",
    "Because of its ubiquity in computational linear algebra and data science. we will spend a bit of time to understand the SVD and its applications (but not the specific algorithms).  But first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamentals\n",
    "\n",
    "**Objectives**\n",
    "* Understand basic linear-algebraic operations and their **computational costs**\n",
    "* Understand Numpy implementation (and performance) for Linear Algebra\n",
    "* Understand Singular vs Invertible matrices\n",
    "* Understand Orthonormal vectors and $Q$ matrices\n",
    "* Understand vector and Matrix norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix-Vector Multiplication \n",
    "\n",
    "One of the most basic operations we can perform with matrices is to multiply them by a vector which maps a vector to a vector.  There are multiple ways to interpret and compute the matrix-vector product $A \\mathbf{x}$.\n",
    "\n",
    "#### index notation\n",
    "$$\n",
    "    b_i = \\sum^n_{j=1} a_{ij} x_j \\quad \\text{where}\\quad i = 1, \\ldots, m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### row picture (dot products)\n",
    "We can also consider matrix-vector multiplication as a sequence of inner products (dot-products between the rows of $A$ and the vector $\\mathbf{x}$.  \n",
    "\\begin{align}\n",
    "    \\mathbf{b} &= A \\mathbf{x}, \\\\\n",
    "     &= \n",
    "    \\begin{bmatrix}  \\mathbf{a}_1^T \\mathbf{x} \\\\ \\mathbf{a}_2^T \\mathbf{x} \\\\ \\vdots \\\\ \\mathbf{a}_m^T \\mathbf{x}\\end{bmatrix}\n",
    "\\end{align}\n",
    "where $\\mathbf{a}_i^T$ is the $i$th **row** of $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### column picture\n",
    "\n",
    "An alternative (and entirely equivalent way) to write the matrix-vector product is as a linear combination of the columns of $A$ where each column's weighting is $x_j$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{b} &= A \\mathbf{x}, \\\\\n",
    "    &= \n",
    "    \\begin{bmatrix}  &  &  &  \\\\  &  &  &  \\\\ \\mathbf{a}_1 & \\mathbf{a}_2 & \\cdots & \\mathbf{a}_n \\\\  &  &  &   \\\\  &  &  &  \\end{bmatrix}\n",
    "    \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}, \\\\\n",
    "  &= x_1 \\mathbf{a}_1 + x_2 \\mathbf{a}_2 + \\cdots + x_n \\mathbf{a}_n.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This view will be useful later when we are trying to interpret various types of matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Operation Counts\n",
    "No matter how you compute $A\\mathbf{x}$,  the total number of operations is the same (for a dense matrix $A$). \n",
    "The row view however is convenient for calculating the **Operation counts** required for $A\\mathbf{x}$.  \n",
    "\n",
    "If $A\\in\\mathbb{C}^{m\\times n}$ and $\\mathbf{x}\\in\\mathbb{C}^n$.  Then just counting the number of multiplications involved to compute $A\\mathbf{x}$ is $O(??)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One important property of the matrix-vector product is that is a **linear** operation, also known as a **linear operator**.  This means that the for any $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{C}^n$ and any $c \\in \\mathbb{C}$ we know that\n",
    "\n",
    "1. $A (\\mathbf{x} + \\mathbf{y}) = A\\mathbf{x} + A\\mathbf{y}$\n",
    "1. $A\\cdot (c\\mathbf{x}) = c A \\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example:  Numerical matrix-vector multiply\n",
    "\n",
    "Write a matrix-vector multiply function and check it with the appropriate `numpy` routine.  Also verify the linearity of the matrix-vector multiply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Mat-vec in index notation (the long way...don't do this)\n",
    "def matrix_vector_product_index(A, x):\n",
    "    m, n = A.shape\n",
    "    b = numpy.zeros(m)\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            b[i] += A[i, j] * x[j]\n",
    "    return b\n",
    "\n",
    "# Mat-vec by row picture (still don't do this)\n",
    "def matrix_vector_product_row(A, x):\n",
    "    m, n = A.shape\n",
    "    b = numpy.zeros(m)\n",
    "    # loop on rows\n",
    "    for i in range(m):\n",
    "        b[i] = A[i, :].dot(x)\n",
    "    return b\n",
    "\n",
    "# Mat-vec by column picture (still don't do this)\n",
    "def matrix_vector_product_col(A, x):\n",
    "    m, n = A.shape\n",
    "    b = numpy.zeros(m)\n",
    "    # loop over columns\n",
    "    for j in range(n):\n",
    "            b += A[:, j] * x[j]\n",
    "    return b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Check equivalence\n",
    "\n",
    "first set up some large random matrices and vectors and compare to numpy built in .dot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "m = 1000\n",
    "n = 1000\n",
    "A = numpy.random.uniform(size=(m,n))\n",
    "x = numpy.random.uniform(size=(n))\n",
    "y = numpy.random.uniform(size=(n))\n",
    "c = numpy.random.uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "funcs = [ matrix_vector_product_index,  matrix_vector_product_row,  matrix_vector_product_col ] \n",
    "for f in funcs:\n",
    "    b = f(A, x)\n",
    "    print('{}(A,x) = A.dot(x)? {}'.format(f.__name__, numpy.allclose(b, A.dot(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Check Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for f in funcs:\n",
    "    print('{}(A,x+y) = Ax + Ay is {}'.format(f.__name__, \n",
    "                                         numpy.allclose(f(A, (x + y)), f(A, x) + f(A, y))))\n",
    "    print('{}(A,cx) = cAx  is {}\\n'.format(f.__name__, \n",
    "                                         numpy.allclose(f(A, c * x), c*f(A, x))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Check Timing/performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for f in funcs:\n",
    "    print(f.__name__,end='\\t')\n",
    "    %timeit matrix_vector_product_index(A,x)\n",
    "\n",
    "print('numpy.dot(A,x)',end='\\t\\t\\t')\n",
    "%timeit A.dot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix-Matrix Multiplication\n",
    "\n",
    "The matrix product with another matrix $ C=AB$ is defined as\n",
    "$$\n",
    "    c_{ij} = \\sum^m_{k=1} a_{ik} b_{kj} = \\mathbf{a}_i^T\\mathbf{b}_j\n",
    "$$\n",
    "\n",
    "i.e. each component of $C$ is a dot-product between the $i$th row of $A$ and the $j$th column of $B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As with matrix-vector multiplication, Matrix-matrix multiplication can be thought of multiple ways\n",
    "\n",
    "* $m\\times p$ dot products (each with $n$ flops)\n",
    "* $A$ multiplying the columns of $B$\n",
    "$$\n",
    "    C = AB = \\begin{bmatrix} \n",
    "                A\\mathbf{b}_1 & A\\mathbf{b}_2 & \\ldots & A\\mathbf{b}_p\\\\ \n",
    "             \\end{bmatrix}\n",
    "$$\n",
    "* Linear combinations of the rows of $B$\n",
    "$$\n",
    "C = AB = \\begin{bmatrix} \n",
    "                \\mathbf{a}_1^T B \\\\ \\mathbf{a}_2^T B \\\\ \\vdots \\\\ \\mathbf{a}_m^T B\\\\ \n",
    "             \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Questions\n",
    "* What are the dimensions of $A$ and $B$ so that the multiplication works?\n",
    "* What are the Operations Counts for Matrix-Matrix Multiplication?\n",
    "* Comment on the product $\\mathbf{c}=(AB)\\mathbf{x}$ vs. $\\mathbf{d} = A(B\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example:  Outer Product\n",
    "\n",
    "The product of two vectors $\\mathbf{u} \\in \\mathbb{C}^m$ and $\\mathbf{v} \\in \\mathbb{C}^n$ is a $m \\times n$ matrix where the columns are the vector $u$ multiplied by the corresponding value of $v$:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{u} \\mathbf{v}^T &= \n",
    "    \\begin{bmatrix}  u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_m  \\end{bmatrix}\n",
    "    \\begin{bmatrix} v_1 & v_2 & \\cdots & v_n \\end{bmatrix}, \\\\\n",
    "    & = \\begin{bmatrix} v_1u_1 & \\cdots & v_n u_1 \\\\ \\vdots &  & \\vdots \\\\ v_1 u_m & \\cdots & v_n u_m \\end{bmatrix}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is useful to think of these as operations on the column vectors, and an equivalent way to express this relationship is \n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{u} \\mathbf{v}^T &=\n",
    "    \\begin{bmatrix}  \\\\ \\mathbf{u} \\\\ \\\\  \\end{bmatrix}\n",
    "    \\begin{bmatrix} v_1 & v_2 & \\cdots & v_n \\end{bmatrix}, \\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}  &  &  &  \\\\  &  &  &  \\\\ \\mathbf{u}v_1  & \\mathbf{u} v_2  & \\cdots & \\mathbf{u} v_n  \\\\  &  &  & \\\\  &  &  &  \\end{bmatrix}, \\\\\n",
    "    & = \\begin{bmatrix} v_1u_1 & \\cdots & v_n u_1 \\\\ \\vdots &  & \\vdots \\\\ v_1 u_m & \\cdots & v_n u_m \\end{bmatrix}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Or each column is just a scalar multiple of $\\mathbf{u}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Alternatively you can think of this as the rows are all just scalar multiples of $\\mathbf{v}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### rank 1 updates\n",
    "\n",
    "We call any matrix of the form $\\mathbf{u}\\mathbf{v}^T$ a \"rank one matrix\"  ( because its rank r=?).  These sort of matrix operations are very common in numerical algorithms for orthogonalization, eigenvalues and the original page-rank algorithm for google.  Again, the order of operations is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Comment on the difference in values and operation counts between\n",
    "\n",
    "$$\n",
    "    \\mathbf{y} = (\\mathbf{u}\\mathbf{v}^T)\\mathbf{x}\n",
    "$$\n",
    "\n",
    "and \n",
    "$$\n",
    "    \\tilde{\\mathbf{y}} = \\mathbf{u}(\\mathbf{v}^T\\mathbf{x})\n",
    "$$\n",
    "for $\\mathbf{u}$, $\\mathbf{v}$, $\\mathbf{x}$, $\\mathbf{y}$, $\\tilde{\\mathbf{y}}\\in\\mathbb{R}^n$, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Check \n",
    "\n",
    "Set up some random vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "m = 10000\n",
    "n = 10000\n",
    "u = numpy.random.rand(m)\n",
    "v = numpy.random.rand(n)\n",
    "x = numpy.random.rand(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Check Equivalence of numbers \n",
    "\n",
    "show \n",
    "$$\n",
    "    (\\mathbf{u}\\mathbf{v}^T)\\mathbf{x} = \\mathbf{u}(\\mathbf{v}^T\\mathbf{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "b = numpy.outer(u,v).dot(x)\n",
    "c = u.dot(v.dot(x))\n",
    "d = u*v.dot(x)\n",
    "print('Max Difference = {}'.format(numpy.max(numpy.abs(b-c))))\n",
    "print('Max Difference = {}'.format(numpy.max(numpy.abs(b-d))))\n",
    "print('Max Difference = {}'.format(numpy.max(numpy.abs(c-d))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Check Timing/performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# timing\n",
    "print('(uv^T).dot(x)',end='\\t\\t')\n",
    "%timeit numpy.outer(u,v).dot(x)\n",
    "print('\\n(u.dot(v.dot(x))',end='\\t')\n",
    "%timeit u.dot(v.dot(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Example:  Upper Triangular Multiplication\n",
    "\n",
    "Consider the multiplication of a matrix $A \\in \\mathbb{C}^{m\\times n}$ and the **upper-triangular** matrix $R$ defined as the $n \\times n$ matrix with entries $r_{ij} = 1$ for $i \\leq j$ and $r_{ij} = 0$ for $i > j$.  The product can be written as\n",
    "$$\n",
    "    \\begin{bmatrix}  \\\\  \\\\ \\mathbf{b}_1 & \\cdots & \\mathbf{b}_n \\\\ \\\\ \\\\ \\end{bmatrix} = \\begin{bmatrix} \\\\ \\\\  \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\ \\\\ \\\\ \\end{bmatrix} \\begin{bmatrix} 1 & \\cdots & 1 \\\\  & \\ddots & \\vdots \\\\  &  & 1 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The columns of $B$ are then\n",
    "$$\n",
    "    \\mathbf{b}_j = A \\mathbf{r}_j = \\sum^j_{k=1} \\mathbf{a}_k\n",
    "$$\n",
    "so that $\\mathbf{b}_j$ is the sum of the first $j$ columns of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: Write Matrix-Matrix Multiplication\n",
    "\n",
    "Write a function that computes matrix-matrix multiplication and demonstrate the following properties:\n",
    "1. $A (B + C) = AB + AC$ (for square matrices))\n",
    "1. $A (cB) = c AB$ where $c \\in \\mathbb{C}$\n",
    "1. $AB \\neq BA$ in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def matrix_matrix_product(A, B):\n",
    "    C = numpy.zeros((A.shape[0], B.shape[1]))\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(B.shape[1]):\n",
    "            for k in range(A.shape[1]):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    return C\n",
    "\n",
    "\n",
    "m = 4\n",
    "n = 4\n",
    "p = 4\n",
    "A = numpy.random.uniform(size=(m, n))\n",
    "B = numpy.random.uniform(size=(n, p))\n",
    "C = numpy.random.uniform(size=(m, p))\n",
    "c = numpy.random.uniform()\n",
    "print(numpy.allclose(matrix_matrix_product(A, B), numpy.dot(A, B)))\n",
    "print(numpy.allclose(matrix_matrix_product(A, (B + C)), matrix_matrix_product(A, B) + matrix_matrix_product(A, C)))\n",
    "print(numpy.allclose(matrix_matrix_product(A, c * B), c*matrix_matrix_product(A, B)))\n",
    "print(numpy.allclose(matrix_matrix_product(A, B), matrix_matrix_product(B, A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### NumPy Products\n",
    "\n",
    "NumPy and SciPy contain routines that are optimized to perform matrix-vector and matrix-matrix multiplication.  Given two `ndarray`s you can take their product by using the `dot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "# Matrix vector with identity\n",
    "I = numpy.identity(n)\n",
    "x = numpy.random.random(n)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('x = Ix is {}\\n'.format(numpy.allclose(x, numpy.dot(I, x))))\n",
    "print('x - I.dot(x) = {}\\n'.format(x-I.dot(x)))\n",
    "print('I*x = \\n{}'.format(I*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Matrix vector product\n",
    "m = 5\n",
    "A = numpy.random.random((m, n))\n",
    "print(numpy.dot(A, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Matrix matrix product\n",
    "B = numpy.random.random((n, m))\n",
    "print(numpy.dot(A, B))\n",
    "print()\n",
    "print(A.dot(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Check non-commutative property of matrix-matrix multiplication\n",
    "\n",
    "It's easy to demonstrate the non-commutative nature of Matrix-Matrix multiplication with *almost* any two matrices $A$ and $B$.  For a clear demonstration,  consider the two matrices $D$ a diagonal matrix and $A$ a matrix of all 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "N=5\n",
    "D = numpy.diag(numpy.array(range(1,N+1)))\n",
    "A = numpy.ones((N,N))\n",
    "print(D,'\\n')\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(D.dot(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(A.dot(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Range and Null-Space\n",
    "\n",
    "#### Range\n",
    "- The **range** of a matrix $A \\in \\mathbb R^{m \\times n}$ (similar to any function), denoted as $\\text{range}(A)$, is the set of vectors that can be expressed as $A x$ for $x \\in \\mathbb R^n$.  \n",
    "- We can also then say that that $\\text{range}(A)$ is the space **spanned** by the columns of $A$.  In other words the columns of $A$ provide a basis for $\\text{range}(A)$, also called the **column space** of the matrix $A$.  \n",
    "- $C(A)$ controls the **existence** of solutions to $A\\mathbf{x}=\\mathbf{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Null-Space\n",
    "- Similarly the **null-space** of a matrix $A$, denoted $\\text{null}(A)$ is the set of vectors $\\mathbf{x}$ that satisfy $A \\mathbf{x} = \\mathbf{0}$.\n",
    "- $N(A)$ controls the **uniqueness** of solutions to $A\\mathbf{x}=\\mathbf{b}$\n",
    "- A similar concept is the **rank** of the matrix $A$, denoted as $\\text{rank}(A)$, is the dimension of the column space.  A matrix $A$ is said to have **full-rank** if $\\text{rank}(A) = \\min(m, n)$.  This property also implies that the matrix mapping is **one-to-one**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inverse\n",
    "\n",
    "A **non-singular** or **invertible** matrix is characterized as a square matrix with full-rank.  This is related to why we know that the matrix is one-to-one, we can use it to transform a vector $x$ and using the inverse, denoted $A^{-1}$, we can map it back to the original matrix.  The familiar definition of this is\n",
    "\\begin{align*}\n",
    "    A \\mathbf{x} &= \\mathbf{b}, \\\\\n",
    "    A^{-1} A \\mathbf{x} & = A^{-1} \\mathbf{b}, \\\\\n",
    "    x &=A^{-1} \\mathbf{b}.\n",
    "\\end{align*}\n",
    "Since $A$ has full rank, its columns form a basis for $\\mathbb{R}^m$ and the vector $\\mathbf{b}$ must be in the column space of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are a number of important properties of an invertible matrix $A$.  Here we list them as the following equivalent statements\n",
    "1. $A$ has a *unique* inverse $A^{-1}$ such that  $A^{-1}A=AA^{-1}=I$\n",
    "1. $\\text{rank}(A) = m$\n",
    "1. $\\text{range}(A) = \\mathbb{C}^m$\n",
    "1. $\\text{null}(A) = \\mathbf{0}$\n",
    "1. 0 is not an eigenvalue of $A$\n",
    "1. $\\text{det}(A) \\neq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example:  Properties of invertible matrices\n",
    "\n",
    "Show that given an invertible matrix that the rest of the properties hold.  Make sure to search the `numpy` packages for relevant functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "m = 5\n",
    "for n in range(100):\n",
    "    A = numpy.random.uniform(size=(m, m))\n",
    "    if numpy.linalg.det(A) != 0:\n",
    "        break\n",
    "        \n",
    "print('A^{{-1}}*A = \\n\\n{}\\n'.format(numpy.dot(numpy.linalg.inv(A), A)))\n",
    "print('rank(A) = {}\\n'.format(numpy.linalg.matrix_rank(A)))\n",
    "print(\"N(A)= {}\\n\".format(numpy.linalg.solve(A, numpy.zeros(m))))\n",
    "print(\"Eigenvalues = {}\".format(numpy.linalg.eigvals(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Orthogonal Vectors and Matrices\n",
    "\n",
    "Orthogonality is a very important concept in linear algebra that forms the basis of many of the modern methods used in numerical computations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two vectors are said to be *orthogonal* if their **inner-product** or **dot-product** defined as\n",
    "$$\n",
    "    < \\mathbf{x}, \\mathbf{y} > \\equiv (\\mathbf{x}, \\mathbf{y}) \\equiv \\mathbf{x}^T\\mathbf{y} \\equiv \\mathbf{x} \\cdot \\mathbf{y} = \\sum^m_{i=1} x_i y_i = 0\n",
    "$$\n",
    "Here we have shown the various notations you may run into (the inner-product is in-fact a general term for a similar operation for mathematical objects such as functions).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If $\\langle \\mathbf{x},\\mathbf{y} \\rangle = 0$ then we say $\\mathbf{x}$ and $\\mathbf{y}$ are orthogonal.  The reason we use this terminology is that the inner-product of two vectors can also be written in terms of the angle between them where\n",
    "\n",
    "$$\n",
    "    \\cos \\theta = \\frac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{||\\mathbf{x}||_2~||\\mathbf{y}||_2}\n",
    "$$\n",
    "\n",
    "and $||\\mathbf{x}||_2$ is the Euclidean ($\\ell^2$) norm of the vector $\\mathbf{x}$, which we can interpret as the *length* of a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can write the 2-norm or length of a vector  in terms of the inner-product as well as\n",
    "$$\n",
    "    ||\\mathbf{x}||_2^2 = \\langle \\mathbf{x}, \\mathbf{x} \\rangle = \\mathbf{x}^T\\mathbf{x} = \\sum^m_{i=1} |x_i|^2.\n",
    "$$\n",
    "\n",
    "$$\n",
    "    ||\\mathbf{x}||_2 = \\sqrt{\\langle \\mathbf{x}, \\mathbf{x} \\rangle}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The generalization of the inner-product to complex spaces is defined as\n",
    "$$\n",
    "    \\langle x, y \\rangle = \\sum^m_{i=1} x_i^* y_i\n",
    "$$\n",
    "where $x_i^*$ is the complex-conjugate of the value $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Orthonormality\n",
    "\n",
    "Taking this idea one step further we can say a set of vectors $\\mathbf{x} \\in X$ are orthogonal to $\\mathbf{y} \\in Y$ if $\\forall \\mathbf{x},\\mathbf{y}$ $< \\mathbf{x}, \\mathbf{y} > = 0$.  If $\\forall \\mathbf{x},\\mathbf{y}$ $||\\mathbf{x}|| = 1$ and $||\\mathbf{y}|| = 1$ then they are also called orthonormal. \n",
    "\n",
    "Note that we dropped the 2 as a subscript to the notation for the norm of a vector. Later we will explore other ways to define a norm of a vector other than the Euclidean norm defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another concept that is related to orthogonality is linear-independence.  A set of vectors $\\mathbf{x} \\in X$ are **linearly independent** if $\\forall \\mathbf{x} \\in X$ that each $\\mathbf{x}$ cannot be written as a linear combination of the other vectors in the set $X$. \n",
    "\n",
    "\n",
    "\n",
    "An equivalent statement is that given a set of $n$ vectors $\\mathbf{x}_i$,  the only  set of scalars $c_i$ that satisfies\n",
    "$$\n",
    "    \\sum_{i=1}^n c_i\\mathbf{x}_i = \\mathbf{0}\n",
    "$$\n",
    "is if $c_i=0$ for all $i\\in[1,n]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Show that if a set of vectors are orthonormal,  they must be linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This can be related directly through the idea of projection.  If we have a set of vectors $\\mathbf{x} \\in X$ we can project another vector $\\mathbf{v}$ onto the vectors in $X$ by using the inner-product.  This is especially powerful if we have a set of **orthogonal** vectors $X$, which are said to **span** a space (or provide a **basis** for a space), s.t. any vector in the space spanned by $X$ can be expressed as a linear combination of the basis vectors $X$\n",
    "$$\n",
    "    \\mathbf{v} = \\sum^n_{i=1} \\, \\langle \\mathbf{v}, \\mathbf{x}_i \\rangle \\, \\mathbf{x}_i.\n",
    "$$\n",
    "Note if $\\mathbf{v} \\in X$ that \n",
    "$$\n",
    "    \\langle \\mathbf{v}, \\mathbf{x}_i \\rangle = 0 \\quad \\forall \\mathbf{x}_i \\in X \\setminus \\mathbf{v}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Looping back to matrices, the column space of a matrix is spanned by its linearly independent columns.  Any vector $v$ in the column space can therefore be expressed via the equation above.  A special class of matrices are called **unitary** matrices when complex-valued and **orthogonal** when purely real-valued if the columns of the matrix are orthonormal to each other.  Importantly this implies that for a unitary matrix $Q$ we know the following\n",
    "\n",
    "1. $Q^* = Q^{-1}$\n",
    "1. $Q^*Q = I$\n",
    "\n",
    "where $Q^*$ is called the **adjoint** (or Hermitian) of $Q$.  The adjoint is defined as the transpose of the original matrix with the entries being the complex conjugate of each entry as the notation implies.  Note, if $Q\\in\\mathbb{R}^{n\\times n}$ then $Q^*=Q^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As an example if we have the matrix\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    Q &= \\begin{bmatrix} q_{11} & q_{12} \\\\ q_{21} & q_{22} \\\\ q_{31} & q_{32} \\end{bmatrix} \\quad \\text{then} \\\\\n",
    "    Q^* &= \\begin{bmatrix} q^*_{11} & q^*_{21} & q^*_{31} \\\\ q^*_{12} & q^*_{22} & q^*_{32} \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "The important part of being an unitary matrix is that the projection onto the column space of the matrix $Q$ preserves geometry in an Euclidean sense, i.e. preserves the Cartesian distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vector and Matrix Norms and the condition number\n",
    "\n",
    "The following sections will lay out the definitions and computation required to calculate a range of vector and matrix norms which provide a measure of the \"size\" of an object or the distance in some vector space.\n",
    "\n",
    "In the context of Numerical Linear algebra,  norms are essential for defining a key property of a matrix,  the \"condition number\".  In infinite precision, a square matrix is either invertible or singular,  however, in finite precision, even an invertible matrix can behave poorly if it is 'ill-conditioned', i.e. it is almost singular. \n",
    "\n",
    "We need a quantitiative measure of how \"near-singular\" a matrix is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why not the Determinant $|A|$?\n",
    "\n",
    "We know that if a matrix $A$ is singular, $|A|=0$.  But what if $|A|$ is small?\n",
    "\n",
    "Turns out even a perfectly invertible, well-conditioned matrix can have arbitrarily small determinant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Consider the two diagonal matrices \n",
    "\n",
    "$$\n",
    "    I = \\begin{bmatrix} 1 & & \\\\ & 1 & \\\\ & & 1\\\\ \\end{bmatrix}\\quad\\text{and}\\quad\n",
    "    \\epsilon I = \\begin{bmatrix} \\epsilon & & \\\\ & \\epsilon & \\\\ & & \\epsilon \\\\ \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "by definition, $|I|=1$,  but $|\\epsilon I| = ??$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "More generally if $I\\in\\mathbb{R}^{n\\times n}$, $|\\epsilon I| = ??$.  Yet all these matrices are diagonal and easily inverted (i.e.  $(\\epsilon I )^{-1} = (1/\\epsilon) I$).  Thus we need something better (the condition number). But to get there requires developing important ideas about vector and matrix norms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Vector Norms\n",
    "\n",
    "Norms (and also measures) provide a means for measure the \"size\" or distance in a space.  In general a norm is a function, denoted by $||\\cdot||$, that maps $\\mathbb{C}^m \\rightarrow \\mathbb{R}$.  In other words we stick in a multi-valued object and get a single, real-valued number out the other end.  \n",
    "\n",
    "All norms satisfy the properties:\n",
    "\n",
    "1. $||\\mathbf{x}|| \\geq 0$\n",
    "2. $||\\mathbf{x}|| = 0$ only if $\\mathbf{x} = \\mathbf{0}$\n",
    "3. $||\\mathbf{x} + \\mathbf{y}||\\leq ||\\mathbf{x}|| + ||\\mathbf{y}||$ (triangle inequality)\n",
    "4. $||c \\mathbf{x}|| = |c|||\\mathbf{x}||$ where $c \\in \\mathbb{C}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are a number of relevant norms that we can define beyond the Euclidean norm, also know as the 2-norm or $\\ell_2$ norm:\n",
    "\n",
    "* $\\ell_1$ norm:\n",
    "$$\n",
    "    ||\\mathbf{x}||_1 = \\sum^m_{i=1} |x_i|,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\ell_2$ norm:\n",
    "$$\n",
    "    ||\\mathbf{x}||_2 = \\left( \\sum^m_{i=1} |x_i|^2 \\right)^{1/2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\ell_p$ norm:\n",
    "$$\n",
    "    ||\\mathbf{x}||_p = \\left( \\sum^m_{i=1} |x_i|^p \\right)^{1/p}, \\quad \\quad 1 \\leq p < \\infty,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $\\ell_\\infty$ norm:\n",
    "$$\n",
    "    ||\\mathbf{x}||_\\infty = \\max_{1\\leq i \\leq m} |x_i|,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "1. weighted $\\ell_p$ norm:\n",
    "$$\n",
    "    ||\\mathbf{x}||_{W_p} = \\left( \\sum^m_{i=1} |w_i x_i|^p \\right)^{1/p}, \\quad \\quad 1 \\leq p < \\infty,\n",
    "$$\n",
    "\n",
    "These are also related to other norms denoted by capital letters ($L_2$ for instance).  In this case we use the lower-case notation to denote finite or discrete versions of the infinite dimensional counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Example:  Comparisons Between Norms\n",
    "\n",
    "Compute the norms given some vector $\\mathbf{x}$ and compare their values.  Verify the properties of the norm for one of the norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def pnorm(x, p):\n",
    "    \"\"\" return the vector p norm of a vector\n",
    "    parameters:\n",
    "    -----------\n",
    "    x: numpy array\n",
    "        vector\n",
    "    p: float or numpy.inf\n",
    "        value of p norm such that ||x||_p = (sum(|x_i|^p))^{1/p} for p< inf\n",
    "        for infinity norm return max(abs(x))\n",
    "    returns:\n",
    "    --------\n",
    "    pnorm: float\n",
    "        pnorm of x\n",
    "    \"\"\"\n",
    "    if p == numpy.inf:\n",
    "        norm = numpy.max(numpy.abs(x))\n",
    "    else:\n",
    "        norm = numpy.sum(numpy.abs(x)**p)**(1./p)\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "m = 10\n",
    "p = 4\n",
    "x = numpy.random.uniform(size=m)        \n",
    "ell_1 = pnorm(x, 1)\n",
    "ell_2 = pnorm(x, 2)\n",
    "ell_p = pnorm(x, p)\n",
    "ell_infty = pnorm(x, numpy.inf)\n",
    "\n",
    "print('x = {}'.format(x))\n",
    "print()\n",
    "print(\"L_1   = {}\\nL_2   = {}\\nL_{}   = {}\\nL_inf = {}\".format(ell_1, ell_2, p, ell_p, ell_infty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "y = numpy.random.uniform(size=m)\n",
    "print()\n",
    "print(\"Properties of norms:\")\n",
    "\n",
    "print('x = {}'.format(x))\n",
    "print('y = {}\\n'.format(y))\n",
    "p = 2\n",
    "print('||x+y||_{p}         = {nxy}\\n||x||_{p} + ||y||_{p} = {nxny}'.format(\n",
    "    p=p,nxy=pnorm(x+y, p), nxny=pnorm(x, p) + pnorm(y, p)))\n",
    "c = -0.1\n",
    "print('||c x||_{} = {}'.format(p,pnorm(c * x, p)))\n",
    "print('|c||x||_{} = {}'.format(p,abs(c) * pnorm(x, p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Geometric Interpretation\n",
    "\n",
    "For every $p-$norm we can define a set of 'unit vectors' that is the set of all vectors in $\\mathbb{R}^n$ with $||\\mathbf{x}||_p = 1$.  \n",
    "\n",
    "For example in $\\mathbb{R}^2$, the unit spheres in the 1-,2- and $\\infty$-norm look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "# Note: that this code is a bit fragile to angles that go beyond pi\n",
    "# due to the use of arccos.\n",
    "    \n",
    "head_width = 0.1\n",
    "head_length = 1.5 * head_width\n",
    "def draw_unit_vectors(axes, A, head_width=0.1):\n",
    "    head_length = 1.5 * head_width\n",
    "    image_e = numpy.empty(A.shape)\n",
    "    angle = numpy.empty(A.shape[0])\n",
    "    image_e[:, 0] = numpy.dot(A, numpy.array((1.0, 0.0)))\n",
    "    image_e[:, 1] = numpy.dot(A, numpy.array((0.0, 1.0)))\n",
    "    for i in range(A.shape[0]):\n",
    "        angle[i] = numpy.arccos(image_e[0, i] / numpy.linalg.norm(image_e[:, i], ord=2))\n",
    "        axes.arrow(0.0, 0.0, image_e[0, i] - head_length * numpy.cos(angle[i]), \n",
    "                             image_e[1, i] - head_length * numpy.sin(angle[i]), \n",
    "                             head_width=head_width, color='b', alpha=0.5)\n",
    "        \n",
    "# comparison of norms\n",
    "# ============\n",
    "# 1-norm\n",
    "# Unit-ball\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "#fig.suptitle(\"1-Norm: $||A||_1 = {}$\".format(numpy.linalg.norm(A,ord=1)), fontsize=16)\n",
    "\n",
    "theta=numpy.linspace(0., 2.*numpy.pi,100)\n",
    "axes = fig.add_subplot(1, 1, 1, aspect='equal')\n",
    "axes.plot((1.0, 0.0, -1.0, 0.0, 1.0), (0.0, 1.0, 0.0, -1.0, 0.0), 'r', label='$||\\mathbf{x}||_1=1$')\n",
    "axes.plot(numpy.cos(theta),numpy.sin(theta), 'g', label='$||\\mathbf{x}||_2=1$')\n",
    "axes.plot((1.0, -1.0, -1.0, 1.0, 1.0), (1.0, 1.0, -1.0, -1.0, 1.0), 'b', label='$||\\mathbf{x}||_\\infty=1$')\n",
    "draw_unit_vectors(axes, numpy.eye(2))\n",
    "axes.arrow(0.0, 0.0, 1.0 - head_length * numpy.cos(numpy.pi/4.), \n",
    "                     1.0 - head_length * numpy.sin(numpy.pi/4.), \n",
    "                             head_width=head_width, color='k', linestyle='--', alpha=0.5)\n",
    "axes.set_title(\"Unit Ball in 1-,2-,and $\\infty$ norm\")\n",
    "axes.set_xlim((-1.1, 1.1))\n",
    "axes.set_ylim((-1.1, 1.1))\n",
    "axes.grid(True)\n",
    "#axes.legend([ '$||\\mathbf{x}||_1=1$', '$||\\mathbf{x}||_2=1$', '$||\\mathbf{x}||_\\infty=1$'], loc='best')\n",
    "axes.legend(loc='upper center', bbox_to_anchor = (0.5,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And if $\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix}$   then $||\\mathbf{x}||_1 = ??,~~$ \n",
    "  $||\\mathbf{x}||_2 = ??,~~$  and  $||\\mathbf{x}||_\\infty = ??$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Induced Matrix Norms\n",
    "\n",
    "The most direct way to consider a matrix norm is one induced by a vector-norm.  Given a vector norm, we can define a matrix p-norm as the smallest number $C$ that satisfies the inequality\n",
    "$$\n",
    "    ||A \\mathbf{x}||_{p} \\leq C ||\\mathbf{x}||_{p} \\quad\\forall\\quad \\mathbf{x}\\in\\mathbb{C}^n\n",
    "$$\n",
    "or as the supremum of the ratios so that\n",
    "$$\n",
    "    C = ||A||_p = \\sup_{\\mathbf{x}\\in\\mathbb{C}^n ~ \\mathbf{x}\\neq\\mathbf{0}} \\frac{||A \\mathbf{x}||_{p}}{||\\mathbf{x}||_p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "with no loss of generality,  we can restrict $\\mathbf{x}$ to the set of all unit vectors $||\\mathbf{x}||_p=1$ and interpret the matrix p-norm as the maximum distortion of the \"unit sphere\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Properties of all Matrix Norms (induced and  non-induced)\n",
    "\n",
    "In general matrix-norms have the following properties whether they are induced from a vector-norm or not:\n",
    "1. $||A|| \\geq 0$ and $||A|| = 0$ only if $A = 0$\n",
    "1. $||A + B|| \\leq ||A|| + ||B||$ (Triangle Inequality)\n",
    "1. $||c A|| = |c| ||A||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In addition,  all induced p-norms satisfy the product rules\n",
    "1. $||AB|| \\leq ||A||\\,||B||$\n",
    "1. $||A\\mathbf{x}|| \\leq ||A||\\,||\\mathbf{x}||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computation of induced matrix p-norms\n",
    "\n",
    "With a little work it can be shown that\n",
    "\n",
    "1. $||A||_1$ = `numpy.linalg.norm(A, ord=1)` is the maximum 1-norm of the Columns of $A$\n",
    "1. $||A||_2$ = `numpy.linalg.norm(A, ord=2) = max(numpy.linalg.svd(A)[1]` is the maximum Singular Value of $A$\n",
    "1. $||A||_\\infty$ = `numpy.linalg.norm(A, ord=numpy.inf) ` is the maximum 1-norm of the Rows of $A$ (or $||A^T||_1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples\n",
    "\n",
    "#### The Identity Matrix $||I||$\n",
    "$$\n",
    " I = \\begin{bmatrix} 1 & & \\\\ & 1 & \\\\ & & 1\\\\ \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$||I||_1=||I||_2=||I||_\\infty = ??$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for ord in [1,2,numpy.inf]:\n",
    "    print('||I||_{} = {}'.format(ord,numpy.linalg.norm(I,ord=ord)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "#### Diagonal Matrices $||D||$\n",
    "$$\n",
    " D = \\begin{bmatrix} d_1 & & \\\\ & d_2 & \\\\ & & d_3\\\\ \\end{bmatrix} \\quad\\text{e.g.}\\quad\n",
    "  \\begin{bmatrix} 2 & & \\\\ & 1 & \\\\ & & -3\\\\  \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "$||D||_1=|D||_2=||D||_\\infty = ??$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "D = numpy.diag([2,1,-3])\n",
    "for ord in [1,2,numpy.inf]:\n",
    "    print('||D||_{} = {}'.format(ord,numpy.linalg.norm(D,ord=ord)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "#### Orthogonal Matrices $||Q||_2$ where $Q^*Q=I$\n",
    "\n",
    "A fundamental property of $Q$ matrices is that they do not change the length of vectors i.e. \n",
    "$$\n",
    " ||Q\\mathbf{x}||_2^2 = \\mathbf{x}^*Q^*Q\\mathbf{x} = \\mathbf{x}^*\\mathbf{x} = ||\\mathbf{x}||_2^2\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "$$\n",
    "    ||Q||_2 = \\sup_{\\mathbf{x}\\in\\mathbb{C}^n, ||\\mathbf{x}||_2=1}  ||Q\\mathbf{x}||_2 = ??\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Example: Induced Matrix Norms\n",
    "\n",
    "Consider the matrix\n",
    "$$\n",
    "    A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix}.\n",
    "$$\n",
    "Compute the induced-matrix norm of $A$ for the vector norms $\\ell_2$ and $\\ell_\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$\\ell^2$: For both of the requested norms the unit-length vectors $[1, 0]$ and $[0, 1]$ can be used to give an idea of what the norm might be and provide a lower bound.  \n",
    "\n",
    "$$\n",
    "    ||A||_2 = \\sup_{x \\in \\mathbb{R}^n} \\left( ||A \\cdot [1, 0]^T||_2, ||A \\cdot [0, 1]^T||_2 \\right )\n",
    "$$\n",
    "\n",
    "computing each of the norms we have\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} &= \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\\\\n",
    "    \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "which translates into the norms $||A \\cdot [1, 0]^T||_2 = 1$ and $||A \\cdot [0, 1]^T||_2 = 2 \\sqrt{2}$.  This implies that the $\\ell_2$ induced matrix norm of $A$ is at least $||A||_{2} = 2 \\sqrt{2} \\approx 2.828427125$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The exact value of $||A||_2$ can be computed using the spectral radius defined as\n",
    "$$\n",
    "    \\rho(A) = \\max_{i} |\\lambda_i|,\n",
    "$$\n",
    "where $\\lambda_i$ are the eigenvalues of $A$.  With this we can compute the $\\ell_2$ norm of $A$ as\n",
    "$$\n",
    "    ||A||_2 = \\sqrt{\\rho(A^\\ast A)}\n",
    "$$\n",
    "\n",
    "Computing the norm again here we find\n",
    "$$\n",
    "    A^\\ast A = \\begin{bmatrix} 1 & 0 \\\\ 2 & 2 \\end{bmatrix} \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 \\\\ 2 & 8 \\end{bmatrix}\n",
    "$$\n",
    "which has eigenvalues \n",
    "$$\n",
    "    \\lambda = \\frac{1}{2}\\left(9 \\pm \\sqrt{65}\\right )\n",
    "$$\n",
    "so $||A||_2 \\approx 2.9208096$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The actual  induced 2-norm of a matrix can be derived using the Singular Value Decomposition (SVD) and is simply the largest singular value $\\sigma_1$.\n",
    "\n",
    "**Proof**: \n",
    "Given that every Matrix $A\\in\\mathbb{C}^{m\\times n}$ can be factored into its SVD (see notebook 10.1):\n",
    "\n",
    "$$\n",
    "    A = U\\Sigma V^*\n",
    "$$\n",
    "\n",
    "where $U\\in\\mathbb{C}^{m\\times m}$ and $V\\in\\mathbb{C}^{n\\times n}$ are unitary matrices with the property $U^*U=I$ and $V^*V=I$ (of their respective sizes) and $\\Sigma$ is a real diagonal matrix of singular values $\\sigma_1 \\geq\\sigma_2\\geq...\\sigma_n\\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Then the 2-norm squared of a  square matrix is \n",
    "$$\n",
    "    ||A||^2_2 = \\sup_{\\mathbf{x} \\in \\mathbb{C}^n ~ ||\\mathbf{x}||_2 = 1} ||A \\mathbf{x}||_2^2 = \\mathbf{x}^TA^*A\\mathbf{x}\n",
    "$$\n",
    "but $A^*A = V\\Sigma^2V^*$ so\n",
    "\n",
    "\\begin{align}\n",
    "    ||A \\mathbf{x}||_2^2 &= \\mathbf{x}^*V\\Sigma^2V^*\\mathbf{x} \\\\\n",
    "                         &= \\mathbf{y}^*\\Sigma^2\\mathbf{y} \\quad\\mathrm{where}\\quad \\mathbf{y}=V^*\\mathbf{x}\\\\\n",
    "                         &= \\sum_{i=1}^n \\sigma_i^2|y_i|^2\\\\\n",
    "                         &\\leq \\sigma_1^2\\sum_{i=1}^n |y_i|^2 = \\sigma_i^2||\\mathbf{y}||_2\\\\\n",
    "\\end{align}  \n",
    "\n",
    "but if $||\\mathbf{x}||_2 = 1$ (i.e. is a unit vector), then so is $\\mathbf{y}$ because unitary matrices don't change the length of vectors.  So it follows that \n",
    "$$\n",
    "    ||A||_2 = \\sigma_1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "A = numpy.array([[1, 2], [0, 2]])\n",
    "\n",
    "#calculate the SVD(A)\n",
    "U, S, Vt = numpy.linalg.svd(A)\n",
    "\n",
    "print('Singular_values = {}'.format(S))\n",
    "print('||A||_2 = {}'.format(S.max()))\n",
    "print('||A||_2 = {}'.format(numpy.linalg.norm(A, ord=2)))\n",
    "\n",
    "# more fun facts about the SVD\n",
    "#print(U.T.dot(U))\n",
    "#print(Vt.T.dot(Vt))\n",
    "#print(A - numpy.dot(U,numpy.dot(numpy.diag(S),Vt)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Some Quick proofs\n",
    "\n",
    "* The induced 1-norm is the max of the 1-norm of the **columns** of $A$\n",
    "\n",
    "Given\n",
    "$$\n",
    "    A\\mathbf{x} = x_1\\mathbf{a}_1 + x_2\\mathbf{a}_2 + \\ldots + x_n\\mathbf{a}_n \n",
    "$$\n",
    "\n",
    "where $||\\mathbf{x}||_1 = 1$. Then\n",
    "$$\n",
    "\\begin{align}\n",
    "||A \\mathbf{x}||_1 &= ||  x_1\\mathbf{a}_1 + x_2\\mathbf{a}_2 + \\ldots + x_n\\mathbf{a}_n || \\\\\n",
    "    &\\leq |x_1|\\,||\\mathbf{a}_1||_1 + |x_2|\\,||\\mathbf{a}_2||_1 + \\ldots + |x_n|\\,||\\mathbf{a}_n||_1 \\quad\\text{(triangle rule)}\\\\\n",
    "    &\\leq \\max_{1\\leq j\\leq n} ||\\mathbf{a}_j||_1\\sum_{j=1}^n |x_j| =  \\max_{1\\leq j\\leq n} ||\\mathbf{a}_j||_1 ||\\mathbf{x}||_1\\\\\n",
    "    &= \\max_{1\\leq j\\leq n} ||\\mathbf{a}_j||_1\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The induced 2-norm is the maximum singular value (short version...slightly scrappy)\n",
    "Given\n",
    "$$\n",
    "    A = U\\Sigma V^T \n",
    "$$\n",
    "\n",
    "where $U^TU = V^TV = I$ and $\\Sigma$ is a diagonal matrix with diagonal entries $\\sigma_1\\geq\\sigma_2\\geq\\ldots\\geq\\sigma_r>0$\n",
    "\n",
    "then\n",
    "$$\n",
    "\\begin{align}\n",
    "    ||AV||_2 &= ||U\\Sigma||_2\\\\\n",
    "    ||A||_2||V||_2 &= ||U||_2||\\Sigma||_2\\\\\n",
    "    ||A||_2 &\\leq ||\\Sigma||_2 = \\sigma_1 \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The induce $\\infty$-norm is  the max of the 1-norm of **rows** of $A$\n",
    "\n",
    "$$\n",
    "||A \\mathbf{x}||_\\infty = \\max_{1 \\leq i \\leq m} | \\mathbf{a}^*_i \\mathbf{x} | \\leq \\max_{1 \\leq i \\leq m} ||\\mathbf{a}^*_i||_1\n",
    "$$\n",
    "because the largest unit vector on the unit sphere in the $\\infty$ norm is a vector of 1's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "####  Example:\n",
    "\n",
    "$$\n",
    "    A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "$$ ||A||_1 = 4, \\quad ||A||_\\infty = 3$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the 1-norm of A\n",
    "A = numpy.array([ [ 1, 2], [ 0, 2]])\n",
    "normA_1 = numpy.max(numpy.sum(numpy.abs(A), axis=0))\n",
    "print('||A||_1 = {}'.format(normA_1))\n",
    "print('||A||_1 = {}'.format(numpy.linalg.norm(A, ord=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the 2  norm of A\n",
    "normA_2 = numpy.max(numpy.linalg.svd(A, compute_uv=False))\n",
    "print('||A||_2 = {}'.format(normA_2))\n",
    "print('||A||_2 = {}'.format(numpy.linalg.norm(A, ord=2)))\n",
    "U,S, V = numpy.linalg.svd(A)\n",
    "print(U.dot(numpy.diag(S).dot(V)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the infinity norm of A\n",
    "normA_inf = numpy.max(numpy.sum(numpy.abs(A), axis=1))\n",
    "print('||A||_inf = {}'.format(normA_inf))\n",
    "print('||A||_inf = {}'.format(numpy.linalg.norm(A, ord=numpy.inf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Geometric Picture\n",
    "\n",
    "One of the most useful ways to think about matrix norms is as a transformation of a unit-ball to an ellipse.  Depending on the norm in question, the norm will be some combination of the resulting ellipse.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1-Norm: $A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\\\ \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "A = numpy.array([[1, 2], [0, 2]])\n",
    "#============\n",
    "# 1-norm\n",
    "# Unit-ball\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(fig.get_figwidth() * 2)\n",
    "fig.suptitle(\"1-Norm:  $||A||_1 = {}$\".format(numpy.linalg.norm(A,ord=1)), fontsize=16)\n",
    "\n",
    "axes = fig.add_subplot(1, 2, 1, aspect='equal')\n",
    "axes.plot((1.0, 0.0, -1.0, 0.0, 1.0), (0.0, 1.0, 0.0, -1.0, 0.0), 'r')\n",
    "draw_unit_vectors(axes, numpy.eye(2))\n",
    "axes.set_title(\"Unit Ball\")\n",
    "axes.set_xlim((-1.1, 1.1))\n",
    "axes.set_ylim((-1.1, 1.1))\n",
    "axes.grid(True)\n",
    "\n",
    "# Image\n",
    "axes = fig.add_subplot(1, 2, 2, aspect='equal')\n",
    "axes.plot((1.0, 2.0, -1.0, -2.0, 1.0), (0.0, 2.0, 0.0, -2.0, 0.0), 'r')\n",
    "draw_unit_vectors(axes, A, head_width=0.2)\n",
    "\n",
    "axes.set_title(\"Images Under A\")\n",
    "axes.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 2-Norm: $A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\\\ \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============\n",
    "# 2-norm\n",
    "\n",
    "# Unit-ball\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"2-Norm: $||A||_2 = ${:3.4f}\".format(numpy.linalg.norm(A,ord=2)),fontsize=16)\n",
    "fig.set_figwidth(fig.get_figwidth() * 2)\n",
    "\n",
    "axes = fig.add_subplot(1, 2, 1, aspect='equal')\n",
    "axes.add_artist(plt.Circle((0.0, 0.0), 1.0, edgecolor='r', facecolor='none'))\n",
    "draw_unit_vectors(axes, numpy.eye(2))\n",
    "axes.set_title(\"Unit Ball\")\n",
    "axes.set_xlim((-1.1, 1.1))\n",
    "axes.set_ylim((-1.1, 1.1))\n",
    "axes.grid(True)\n",
    "\n",
    "# Image\n",
    "# Compute some geometry\n",
    "u, s, v = numpy.linalg.svd(A)\n",
    "theta = numpy.empty(A.shape[0])\n",
    "ellipse_axes = numpy.empty(A.shape)\n",
    "theta[0] = numpy.arccos(u[0][0]) / numpy.linalg.norm(u[0], ord=2)\n",
    "theta[1] = theta[0] - numpy.pi / 2.0\n",
    "for i in range(theta.shape[0]):\n",
    "    ellipse_axes[0, i] = s[i] * numpy.cos(theta[i])\n",
    "    ellipse_axes[1, i] = s[i] * numpy.sin(theta[i])\n",
    "\n",
    "axes = fig.add_subplot(1, 2, 2, aspect='equal')\n",
    "axes.add_artist(patches.Ellipse((0.0, 0.0), 2 * s[0], 2 * s[1], theta[0] * 180.0 / numpy.pi,\n",
    "                                edgecolor='r', facecolor='none'))\n",
    "for i in range(A.shape[0]):\n",
    "    axes.arrow(0.0, 0.0, ellipse_axes[0, i] - head_length * numpy.cos(theta[i]), \n",
    "                         ellipse_axes[1, i] - head_length * numpy.sin(theta[i]), \n",
    "                         head_width=head_width, color='k')\n",
    "draw_unit_vectors(axes, A, head_width=0.2)\n",
    "axes.set_title(\"Images Under A\")\n",
    "axes.set_xlim((-s[0] + 0.1, s[0] + 0.1))\n",
    "axes.set_ylim((-s[0] + 0.1, s[0] + 0.1))\n",
    "axes.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### $\\infty$-Norm: $A = \\begin{bmatrix} 1 & 2 \\\\ 0 & 2 \\\\ \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# ============\n",
    "# infty-norm\n",
    "# Unit-ball\n",
    "fig = plt.figure()\n",
    "fig.suptitle(\"$\\infty$-Norm: $||A||_\\infty = {}$\".format(numpy.linalg.norm(A,ord=numpy.inf)),fontsize=16)\n",
    "fig.set_figwidth(fig.get_figwidth() * 2)\n",
    "\n",
    "axes = fig.add_subplot(1, 2, 1, aspect='equal')\n",
    "axes.plot((1.0, -1.0, -1.0, 1.0, 1.0), (1.0, 1.0, -1.0, -1.0, 1.0), 'r')\n",
    "draw_unit_vectors(axes, numpy.eye(2))\n",
    "axes.set_title(\"Unit Ball\")\n",
    "axes.set_xlim((-1.1, 1.1))\n",
    "axes.set_ylim((-1.1, 1.1))\n",
    "axes.grid(True)\n",
    "\n",
    "# Image\n",
    "# Geometry - Corners are A * ((1, 1), (1, -1), (-1, 1), (-1, -1))\n",
    "# Symmetry implies we only need two.  Here we just plot two\n",
    "u = numpy.empty(A.shape)\n",
    "u[:, 0] = numpy.dot(A, numpy.array((1.0, 1.0)))\n",
    "u[:, 1] = numpy.dot(A, numpy.array((-1.0, 1.0)))\n",
    "theta[0] = numpy.arccos(u[0, 0] / numpy.linalg.norm(u[:, 0], ord=2))\n",
    "theta[1] = numpy.arccos(u[0, 1] / numpy.linalg.norm(u[:, 1], ord=2))\n",
    "\n",
    "axes = fig.add_subplot(1, 2, 2, aspect='equal')\n",
    "axes.plot((3, 1, -3, -1, 3), (2, 2, -2, -2, 2), 'r')\n",
    "for i in range(A.shape[0]):\n",
    "    axes.arrow(0.0, 0.0, u[0, i] - head_length * numpy.cos(theta[i]), \n",
    "                         u[1, i] - head_length * numpy.sin(theta[i]), \n",
    "                         head_width=head_width, color='k')\n",
    "\n",
    "draw_unit_vectors(axes, A, head_width=0.2)\n",
    "axes.set_title(\"Images Under A\")\n",
    "axes.set_xlim((-4.1, 4.1))\n",
    "axes.set_ylim((-3.1, 3.1))\n",
    "axes.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Cauchy-Schwarz and Hlder Inequalities\n",
    "\n",
    "Computing matrix norms where $p \\neq 1$ or $\\infty$ is more difficult unfortunately.  We have a couple of tools that can be useful however.  \n",
    "\n",
    " - **Cauchy-Schwarz Inequality**:  For the special case where $p=q=2$, for any vectors $\\mathbf{x}$ and $\\mathbf{y}$\n",
    "$$\n",
    "    |\\mathbf{x}^*\\mathbf{y}| \\leq ||\\mathbf{x}||_2 ||\\mathbf{y}||_2\n",
    "$$\n",
    " - **Hlder's Inequality**:  Turns out this holds in general if given a $p$ and $q$ that satisfy $1/p + 1/q = 1$ with $1 \\leq p, q \\leq \\infty$\n",
    "\n",
    "$$\n",
    "    |\\mathbf{x}^*\\mathbf{y}| \\leq ||\\mathbf{x}||_p ||\\mathbf{y}||_q.\n",
    "$$\n",
    "\n",
    "**Note**: this is essentially what we used in the proof of the $\\infty-$norm with $p=1$ and $q=\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The most widely used matrix norm not induced by a vector norm is the **Frobenius norm** defined by\n",
    "$$\n",
    "    ||A||_F = \\left( \\sum^m_{i=1} \\sum^n_{j=1} |A_{ij}|^2 \\right)^{1/2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Invariance under unitary multiplication\n",
    "\n",
    "One important property of the matrix 2-norm (and Frobenius norm) is that multiplication by a unitary matrix does not change the product (kind of like multiplication by 1).  In general for any $A \\in \\mathbb{C}^{m\\times n}$ and unitary matrix $Q \\in \\mathbb{C}^{m \\times m}$ we have\n",
    "\\begin{align*}\n",
    "    ||Q A||_2 &= ||A||_2 \\\\ ||Q A||_F &= ||A||_F.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<sup>1</sup><span id=\"footnoteRegression\"> http://www.utstat.toronto.edu/~brunner/books/LinearModelsInStatistics.pdf</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The condition number\n",
    "\n",
    "Finally we have enough machinery to define the condition number $\\kappa(A)$ (or often $\\mathrm{cond}(A)$) which is simply\n",
    "\n",
    "$$\n",
    "    \\kappa(A) = ||A||\\,||A^{-1}|| \\in [1,\\infty)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Examples for simple matrices\n",
    "\n",
    "* Identity Matrix\n",
    "$$\\kappa(I) = ||I||\\,||I^{-1}|| = ??$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Diagonal Matrix\n",
    "$$\\kappa(D) = ||D||\\,||D^{-1}|| = ??$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 2-norm condition number of a matrix\n",
    "$$\\kappa_2(A) = ||A||_2\\,||A^{-1}||_2 = \\frac{\\sigma_1}{\\sigma_n}$$\n",
    "\n",
    "if $A$ is singular, $\\kappa_2(A) = ??$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Back to our initial example\n",
    "\n",
    "Let \n",
    "$$\n",
    "    A = \\epsilon I \\in \\mathbb{R}^{n\\times n}\n",
    "$$\n",
    "\n",
    "* The Determinant $|A| =\\epsilon^n$ (which can be arbitrarily small)\n",
    "* But\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\kappa(A) &= ||A||\\,||A^{-1}|| \\\\\n",
    "&= ||\\epsilon I||\\,||(1/\\epsilon)I|| \\\\\n",
    "& \\leq |\\epsilon||1/\\epsilon|||I||^2 = 1\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "More generally, it's easy to show that scaling of a matrix does not change its condition number \n",
    "\n",
    "$$\n",
    "    \\kappa(\\alpha A) = ||\\alpha A||\\,||(1/\\alpha)A^{-1}|| =\\kappa(A)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "$$\n",
    "    A = \\begin{bmatrix} 1 & 2 \\\\ 1 +\\epsilon & 2\\\\ \\end{bmatrix}\\quad \\epsilon \\geq \\epsilon_{mach}   \n",
    "$$\n",
    "\n",
    "then\n",
    "$$\n",
    "    A^{-1} = \\frac{-1}{2\\epsilon} \\begin{bmatrix} 2 & -2 \\\\ -(1 +\\epsilon) & 1\\\\ \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So\n",
    "$$\n",
    "    \\kappa_1(A) = ||A||_1||A^{-1}||_1 = \\frac{4}{2\\epsilon}(3 + \\epsilon)\\sim \\frac{6}{\\epsilon}\n",
    "$$\n",
    "\n",
    "which is very ill-conditioned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The condition number and error analysis of $A\\mathbf{x}=\\mathbf{b}$\n",
    "\n",
    "The condition number is important in many parts of analysis of numerical linear algebra, but is easily illustrated in understanding the behavior of solutions to of linear systems\n",
    "\n",
    "Assume that $\\mathbf{x}$ is a solution to $A\\mathbf{x}=\\mathbf{b}$ and we want to understand how a small change in the RHS $\\mathbf{b}$  propagates to errors in $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Consider the perturbed problem\n",
    "$$\n",
    "    A(\\mathbf{x} +\\Delta\\mathbf{x}) = \\mathbf{b} + \\Delta\\mathbf{b}\n",
    "$$\n",
    "which by linearity of Matrix vector multiplication, and that $A\\mathbf{x}=\\mathbf{b}$ implies that\n",
    "\n",
    "$$\n",
    "    A\\Delta\\mathbf{x} = \\Delta\\mathbf{b}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "or\n",
    "$$\n",
    "    \\Delta\\mathbf{x} = A^{-1}\\Delta\\mathbf{b}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given \n",
    "$$\n",
    "    \\Delta\\mathbf{x} = A^{-1}\\Delta\\mathbf{b}\n",
    "$$\n",
    "Taking the norm of both sides implies\n",
    "\n",
    "$$\n",
    "    ||\\Delta\\mathbf{x}|| \\leq  ||A^{-1}||\\,||\\Delta\\mathbf{b}||\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "since $||A\\mathbf{x}|| = ||\\mathbf{b}||$, it follows that\n",
    "\n",
    "$$\n",
    "    \\frac{||\\Delta\\mathbf{x}||}{||A\\mathbf{x}||} \\leq  ||A^{-1}||\\,\\frac{||\\Delta\\mathbf{b}||}{||\\mathbf{b}||}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "or since $||A\\mathbf{x}||\\leq||A||\\,||\\mathbf{x}||$, it follows that\n",
    "\n",
    "$$\n",
    "    \\frac{||\\Delta\\mathbf{x}||}{||A||\\,||\\mathbf{x}||} \\leq  ||A^{-1}||\\,\\frac{||\\Delta\\mathbf{b}||}{||\\mathbf{b}||}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "    \\frac{||\\Delta\\mathbf{x}||}{||\\mathbf{x}||} \\leq  \\kappa(A)\\,\\frac{||\\Delta\\mathbf{b}||}{||\\mathbf{b}||}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "or the relative error in the solution $\\mathbf{x}$ depends on the relative error in the RHS $\\mathbf{b}$ times the condition number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Consider the problem\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix} 1 & 2 \\\\ 1+\\alpha & 2\\\\ \\end{bmatrix}\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix}\n",
    "$$\n",
    "for $\\alpha = O(\\epsilon_{mach})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "alpha = numpy.finfo(float).eps\n",
    "A = numpy.array([ [ 1, 2], [1 + alpha, 2] ])\n",
    "\n",
    "# first version \n",
    "b = numpy.array( [ 1., 1.])\n",
    "x = numpy.linalg.solve(A,b)\n",
    "print('b = {}\\nx = {}'.format(b,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# now perturb b by epsilon\n",
    "bp = numpy.array( [ 1  , 1 - alpha])\n",
    "xp = numpy.linalg.solve(A,bp)\n",
    "print(\"b'={}\\nx' = {}\".format(bp,xp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# and calculate relative error and condition number\n",
    "err_b = numpy.linalg.norm(bp-b)/numpy.linalg.norm(b)\n",
    "err_x = numpy.linalg.norm(xp-x)/numpy.linalg.norm(x)\n",
    "condA = numpy.linalg.cond(A)\n",
    "print('k(A) = {}'.format(condA))\n",
    "print('err_b = {}, err_x = {}, err_x/err_b ={}'.format(err_b, err_x, err_x/err_b))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
